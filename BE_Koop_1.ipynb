{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqIR0dKVqvGyzpilWdOq7y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dandoreyrodriguez/Bayesian_Econometrics/blob/main/BE_Koop_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Class 1: in Bayesian Econometrics**\n",
        "\n",
        "## **Motivation**\n",
        "\n",
        "This is the first in a series of notebooks which document my journey learning Bayesian econometrics. It is intended for people who, like me, have zero background in Bayesian statistics but have a background in statistics/econometrics. The point of this is not to mimic a textbook, but to show how I slowly began to understand things. My hope is that I can closely track the difficulties other new students encounter, and that the reader can learn from them. The reason I figured I had to learn Bayesian statistics is that Bayesian techniques can be applied to general settings. Compare this to a Kalman Filter which required normality. I have heard that it relies on clever computation. Across a variety of fields, it is a good time to learn tools which leverage clever computation. Fittingly, A large goal of this series is therefore to help new students learn to use Bayesian tools in python, chiefly `pyMC`.\n",
        "\n",
        "I learn by doing. After giving a brief summary of Bayesian statistics, I start with the normal linear regression model. This model is what I, and I suspect most others, know best. I show how a Bayesian would attack it, and along the way encounter the main objects of interest in Bayesian econometrics. I do not shy away from doing statistical theory as I am a firm believer that practitioners should not use techniques whose statistical properties they do not understand. If the reader struggles with this, I hope they may find comfort in knowing that I did too.\n",
        "\n",
        "Future notebooks will go over other applications. My goal in this series of is to reach state space time series models and perhaps general equilibrium macro models if I am lucky.\n",
        "\n",
        "Happy learning.\n"
      ],
      "metadata": {
        "id": "Qi8voJCgv0nE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linear Regression**\n",
        "\n",
        "A linear regression model is the most convenient/basic model to think about. I start with this example because it is what I, and most others, know best.\n",
        "\n",
        "Suppose that $\\boldsymbol{y}$ is a $n \\times 1 $ vector of data and $\\boldsymbol{X}$ is a $n \\times k$ design matrix. The $i$th row of this matrix is $\\boldsymbol{x}_i'$, where $\\boldsymbol{x}_i$ is a $k\\times1$ vector which stores each of the features, or independent variables, for that observation. A normal linear regression model is:\n",
        "\n",
        "$$\\boldsymbol{y}= \\boldsymbol{X}\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}, \\, \\boldsymbol{\\epsilon} \\overset{\\text{i.i.d}}{\\sim} \\text{N}(\\boldsymbol{0}, h^{-1} \\boldsymbol{I})$$\n",
        "\n",
        "Normality ensures homoskedasticity and serially-uncorrelatedness of errors.  $h = \\sigma^{-2}$ is a measure of \"tightness\" of errors. $\\boldsymbol{\\beta}$ is the $k \\times 1$ vector of parameters of interest.A *frequentist* treats $\\boldsymbol{\\beta}$ and $h$ as constant (i.e. given) but unknown.\n",
        "\n",
        "But how would a *Bayesian* learn about $\\boldsymbol{\\beta}$ and $h$? Well, start with Bayes' rule!\n",
        "\n",
        "The parameters of inetrest are $\\boldsymbol{\\theta} = [ \\boldsymbol{\\beta}, h]'$. So:\n",
        "\n",
        "$$ f(\\boldsymbol{\\theta} |\\boldsymbol{X}, \\boldsymbol{y}) = \\frac{f(\\boldsymbol{y}|\\boldsymbol{X}, \\boldsymbol{\\theta} )f(\\boldsymbol{\\theta} | \\boldsymbol{X})}{f(\\boldsymbol{y}|\\boldsymbol{X})}$$\n",
        "\n",
        "Now we inspect each object on the right hand side.\n",
        "\n",
        "- $f(\\boldsymbol{y}|\\boldsymbol{X}, \\boldsymbol{\\theta} )$ takes the parameters and explanatory variables as given. That is, it is the likelihood function that we are familiar with from our frequentist training (it is what we use in maximum likelihood!)\n",
        "- $f(\\boldsymbol{\\theta} | \\boldsymbol{X})$ is the prior distribution. It is some probability distribution over the parameter space *before* you see the data\n",
        "\n",
        "Start by considering the likelihood function. In this model, observations are normally distributed:\n",
        "\n",
        "$$\\boldsymbol{y}|\\boldsymbol{X},\\boldsymbol{\\beta},h \\overset{\\text{i.i.d}}{\\sim} \\text{N}(\\boldsymbol{X}\\boldsymbol{\\beta}, h^{-1} \\boldsymbol{I})$$\n",
        "\n",
        "The likelihood function is simlpy the joint probability density function of the data.\n",
        "\n",
        "$$f(\\boldsymbol{y}|\\boldsymbol{X}, \\boldsymbol{\\beta},h) = (2 \\pi)^{-\\frac{n}{2}} h^{\\frac{n}{2}}  \\text{exp}\\bigg( -\\frac{h}{2}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})' (\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})\\bigg),  \\forall \\boldsymbol{\\beta} \\in \\mathbb{R}^n $$\n",
        "\n",
        "The next step is to specify priors. We are going to make a very informed guess. Why will become obvious after the fact. Suppose that our priors are:\n",
        "\n",
        "- $\\boldsymbol{\\beta} \\sim \\text{N}(\\boldsymbol{\\beta}_0, \\boldsymbol{V}_0)\n",
        "$\n",
        "- $h \\sim \\text{Gamma}(\\alpha_0, \\beta_0)$\n",
        "\n",
        "Using the normal and gamma density functions:\n",
        "\n",
        "$$ f(\\boldsymbol{\\beta}) = (2 \\pi)^{-\\frac{k}{2}} |\\boldsymbol{V}_0|^{-\\frac{1}{2}}  \\text{exp}\\bigg( -\\frac{1}{2}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)' \\boldsymbol{V}_0^{-1}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)\\bigg), \\forall \\boldsymbol{\\beta} \\in \\mathbb{R}^{k} $$\n",
        "\n",
        "$$ f(h) = \\frac{1}{\\Gamma(\\alpha_0)}\\beta_0^{\\alpha_0} h^{\\alpha_0-1} \\text{exp}\\bigg(-h\\beta_0\\bigg), \\forall h \\geq 0$$\n",
        "\n",
        "Recall from the first section that the posterior can be written in the following way:\n",
        "\n",
        "$$ f(\\boldsymbol{\\beta}, h | \\boldsymbol{X}, \\boldsymbol{y} ) \\propto f(\\boldsymbol{y}|\\boldsymbol{X}, \\boldsymbol{\\beta},h) f(\\boldsymbol{\\beta}) f(h)$$\n",
        "\n",
        "This gives a long expression:\n",
        "\n",
        "$$ f(\\boldsymbol{\\beta}, h | \\boldsymbol{X}, \\boldsymbol{y} ) \\propto h^{\\frac{n}{2}}  \\text{exp}\\bigg( -\\frac{h}{2}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})' (\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}) -\\frac{1}{2}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)' \\boldsymbol{V}_0^{-1}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)-h\\beta_0\\bigg) h^{\\alpha_0-1} $$\n",
        "\n",
        "However, it is actually easier to spot conditional probabilities than joint probabilities. This is because you can treat the other argument as given and treat it as part of the scaling constant term. Again, to turn to Baye's rule:\n",
        "\n",
        "$$f(\\boldsymbol{\\beta} |h, \\boldsymbol{X}, \\boldsymbol{y} ) = \\frac{f(\\boldsymbol{\\beta}, h | \\boldsymbol{X}, \\boldsymbol{y} )}{f(h| \\boldsymbol{X}, \\boldsymbol{y})} $$\n",
        "\n",
        "The conditional distribution has the same functional form as the joint distribution if you treat $h$ as constant, up to some scaling parameter.\n"
      ],
      "metadata": {
        "id": "cULeDesovw1r"
      }
    }
  ]
}